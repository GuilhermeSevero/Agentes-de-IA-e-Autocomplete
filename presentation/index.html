<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Implemente Agentes de IA e Autocomplete no seu PC</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%);
            color: #1e293b;
            overflow: hidden;
        }

        .presentation {
            width: 100vw;
            height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .slide {
            width: 95%;
            max-width: 1400px;
            height: 95%;
            background: #ffffff;
            border-radius: 8px;
            padding: 80px;
            display: none;
            flex-direction: column;
            justify-content: center;
            border: 1px solid #e2e8f0;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.08);
        }

        .slide.active {
            display: flex;
        }

        h1 {
            font-size: 3.5rem;
            font-weight: 600;
            margin-bottom: 24px;
            text-align: center;
            color: #0f172a;
            line-height: 1.1;
            letter-spacing: -0.02em;
        }

        h2 {
            font-size: 2.5rem;
            font-weight: 500;
            margin-bottom: 48px;
            text-align: center;
            color: #1e293b;
            letter-spacing: -0.01em;
        }

        h3 {
            font-size: 1.5rem;
            font-weight: 500;
            margin-bottom: 32px;
            color: #475569;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .subtitle {
            font-size: 1.4rem;
            text-align: center;
            margin-bottom: 64px;
            color: #475569;
            font-weight: 400;
            letter-spacing: 0.01em;
        }

        .presenter-info {
            text-align: center;
            margin-top: 80px;
            font-size: 1.3rem;
            color: #475569;
            font-weight: 400;
        }

        .bullet-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
            margin: 20px 0;
        }

        .bullet-list {
            list-style: none;
        }

        .bullet-list li {
            font-size: 1.3rem;
            margin-bottom: 18px;
            padding-left: 24px;
            position: relative;
            line-height: 1.5;
            color: #1e293b;
        }

        .bullet-list li::before {
            content: "â€¢";
            position: absolute;
            left: 0;
            color: #3b82f6;
            font-weight: normal;
            font-size: 1.2rem;
        }

        .highlight {
            color: #3b82f6;
            font-weight: 500;
        }

        .code-block {
            background: #f1f5f9;
            border-radius: 6px;
            padding: 32px;
            margin: 24px 0;
            font-family: 'JetBrains Mono', 'SF Mono', Monaco, 'Cascadia Code', monospace;
            font-size: 1.2rem;
            border: 1px solid #e2e8f0;
            white-space: pre-wrap;
            overflow-x: auto;
            color: #0f172a;
            font-weight: 500;
        }

        .demo-section {
            background: #f8fafc;
            border-radius: 8px;
            padding: 24px;
            margin: 24px 0;
            border: 1px solid #e2e8f0;
        }

        .demo-section h4 {
            color: #1e40af;
            font-size: 1.3rem;
            margin-bottom: 12px;
            font-weight: 600;
        }

        .architecture {
            background: #f1f5f9;
            border-radius: 12px;
            padding: 40px;
            margin: 20px 0;
            font-family: 'JetBrains Mono', monospace;
            font-size: 1.3rem;
            text-align: center;
            border: 2px solid #1e293b;
            color: #0f172a;
            font-weight: 600;
        }

        .cta-grid {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }

        .cta-item {
            background: #f8fafc;
            border-radius: 8px;
            padding: 32px;
            text-align: center;
            border: 1px solid #e2e8f0;
        }

        .cta-item h4 {
            font-size: 1.3rem;
            margin-bottom: 20px;
            color: #1e40af;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .navigation {
            position: fixed;
            bottom: 32px;
            right: 32px;
            display: flex;
            gap: 12px;
        }

        .nav-btn {
            background: #ffffff;
            border: 2px solid #1e293b;
            color: #1e293b;
            padding: 16px 32px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 1.1rem;
            font-weight: 600;
            transition: all 0.2s ease;
        }

        .nav-btn:hover {
            background: #1e293b;
            border-color: #1e293b;
            color: #ffffff;
        }

        .slide-counter {
            position: fixed;
            bottom: 32px;
            left: 32px;
            background: #ffffff;
            padding: 16px 24px;
            border-radius: 6px;
            font-size: 1.1rem;
            border: 2px solid #1e293b;
            color: #1e293b;
            font-weight: 600;
        }

        .links-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }

        .link-item {
            background: #f8fafc;
            border-radius: 8px;
            padding: 20px;
            border: 1px solid #e2e8f0;
        }

        .link-item a {
            color: #3b82f6;
            text-decoration: none;
            font-weight: 500;
        }

        .emoji {
            font-size: 1.5em;
            margin-right: 10px;
        }

        .about-me-container {
            display: flex;
            align-items: center;
            gap: 40px;
            margin: 40px 0;
            justify-content: center;
        }

        .avatar-section {
            flex-shrink: 0;
        }

        .avatar {
            width: 300px;
            height: 300px;
            border-radius: 50%;
            object-fit: cover;
            border: 4px solid #3b82f6;
            box-shadow: 0 8px 32px rgba(59, 130, 246, 0.2);
        }

        .about-content {
            flex: 1;
        }

        .about-list {
            list-style: none;
            margin: 32px 0;
        }

        .about-list li {
            font-size: 1.4rem;
            margin-bottom: 24px;
            padding-left: 0;
            color: #1e293b;
            line-height: 1.5;
        }

        .social-links {
            display: flex;
            flex-direction: column;
            gap: 16px;
            margin-top: 40px;
        }

        .social-link {
            display: flex;
            align-items: center;
            font-size: 1.3rem;
            color: #1e40af;
            text-decoration: none;
            font-weight: 600;
        }

        .social-icon {
            width: 24px;
            height: 24px;
            margin-right: 12px;
            fill: currentColor;
        }

        @media (max-width: 768px) {
            .slide {
                padding: 40px 30px;
            }

            h1 {
                font-size: 2.8rem;
                line-height: 1.1;
            }
            h2 {
                font-size: 2.3rem;
                margin-bottom: 32px;
            }
            h3 {
                font-size: 1.6rem;
                margin-bottom: 24px;
            }

            .subtitle {
                font-size: 1.3rem;
                margin-bottom: 48px;
            }

            .bullet-list li {
                font-size: 1.2rem;
                margin-bottom: 16px;
            }

            .code-block {
                font-size: 1.1rem;
                padding: 24px;
            }

            .bullet-grid,
            .cta-grid,
            .links-grid {
                grid-template-columns: 1fr;
                gap: 24px;
            }

            .about-me-container {
                flex-direction: column;
                gap: 32px;
                text-align: center;
            }

            .avatar {
                width: 220px;
                height: 220px;
            }

            .about-list li {
                font-size: 1.3rem;
            }

            .navigation {
                bottom: 20px;
                right: 20px;
            }

            .slide-counter {
                bottom: 20px;
                left: 20px;
                font-size: 1rem;
            }
        }
    </style>
</head>
<body>
    <div class="presentation">

        <!-- Slide 1: Abertura -->
        <div class="slide active">
            <h1>Agentes de IA e Autocomplete no seu PC</h1>
            <div class="subtitle">Crie seu prÃ³prio Jarvis</div>
            <div class="presenter-info">
                <strong>Guilherme Severo</strong><br>
                Development Specialist - Somma IT
            </div>
        </div>

        <!-- Slide 2: Sobre Mim -->
        <div class="slide">
            <h2>Sobre Mim</h2>

            <div class="about-me-container">
                <div class="about-content">
                    <ul class="about-list">
                        <li><strong>Guilherme Henrique Monteiro Severo</strong></li>
                        <li>33 anos</li>
                        <li>Porto Alegre/RS</li>
                        <li>Bacharel em CiÃªncia da ComputaÃ§Ã£o</li>
                        <li>Development Specialist - Somma IT</li>
                    </ul>

                    <div class="social-links">
                        <a href="https://github.com/GuilhermeSevero" class="social-link">
                            <svg class="social-icon" viewBox="0 0 24 24">
                                <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                            </svg>
                            /GuilhermeSevero
                        </a>
                        <a href="https://linkedin.com/in/guilherme-severo" class="social-link">
                            <svg class="social-icon" viewBox="0 0 24 24">
                                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                            </svg>
                            /guilherme-severo
                        </a>
                        <a href="https://instagram.com/guilhermehmsevero" class="social-link">
                            <svg class="social-icon" viewBox="0 0 24 24">
                                <path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/>
                            </svg>
                            @guilhermehmsevero
                        </a>
                    </div>
                </div>
                <div class="avatar-section">
                    <img src="assets/avatar.jpg" alt="Guilherme Severo" class="avatar">
                </div>
            </div>
        </div>

        <!-- Slide 3: IntroduÃ§Ã£o - IA, Models e LLM -->
        <div class="slide">
            <h2>IA, Modelos e LLMs</h2>

            <div class="bullet-grid">
                <div>
                    <h3>O que Ã© IA?</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">InteligÃªncia Artificial:</span> Sistemas que simulam capacidades humanas</li>
                        <li><span class="highlight">Machine Learning:</span> Aprende com dados</li>
                        <li><span class="highlight">Deep Learning:</span> Redes neurais profundas</li>
                    </ul>

                    <h3>LLM - Large Language Model:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Modelo de Linguagem:</span> Entende e gera texto</li>
                        <li><span class="highlight">Treinado em bilhÃµes de textos:</span> Livros, cÃ³digo, web</li>
                        <li><span class="highlight">Transformer:</span> Arquitetura neural moderna</li>
                    </ul>

                    <h3>Modelo vs LLM:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Modelo:</span> Qualquer rede neural treinada</li>
                        <li><span class="highlight">LLM:</span> Modelo especÃ­fico para linguagem (Large Language Model)</li>
                    </ul>
                </div>
                <div>
                    <h3>Como Funciona um LLM?</h3>
                    <div class="code-block"">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Input (Prompt do usuÃ¡rio)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. TokenizaÃ§Ã£o (quebra em tokens)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Processamento Neural                           â”‚
â”‚     â†’ Prediz apenas 1 TOKEN                        â”‚
â”‚     â†’ Modelo processa: [Input + Tokens jÃ¡ gerados] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚    â”‚    â†‘
                         â†“    â””â”€â”€â”€â”€â”˜ Loop de PrediÃ§Ã£o
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Output (Resposta completa gerada)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</div>

                    <div class="demo-section">
                        <h4>ğŸ¤– Exemplos de LLMs:</h4>
                        <p><strong>ProprietÃ¡rios:</strong> GPT-5, Sonnet, Gemini<br>
                        <strong>Open Source:</strong> Llama, Qwen, Mistral, Gemma</p>
                    </div>

                </div>
            </div>
        </div>

        <!-- Slide 4: Conceitos PrÃ¡ticos -->
        <div class="slide">
            <h2>Conceitos PrÃ¡ticos</h2>

            <div class="bullet-grid">
                <div>
                    <h3>1. Tokens:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Tokens:</span> "Palavras" que o modelo entende</li>
                        <li>Texto Ã© quebrado em pedaÃ§os menores</li>
                        <li>32K tokens â‰ˆ 24 mil palavras</li>
                    </ul>

                    <h3>2. Temperatura:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">0.0:</span> Preciso, determinÃ­stico (cÃ³digo, math)</li>
                        <li><span class="highlight">0.7-1.0:</span> Criativo, variado (brainstorm)</li>
                    </ul>

                    <h3>3. Fine-tuning:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Base model:</span> Conhecimento geral</li>
                        <li><span class="highlight">Fine-tuned:</span> Especializado (ex: Qwen-Coder)</li>
                    </ul>
                </div>
                <div>
                    <h3>4. Treinamento vs InferÃªncia:</h3>
                    <div class="code-block">Treinamento (nÃ£o fazemos):
â€¢ Criar modelo do zero
â€¢ Supercomputadores
â€¢ Semanas/meses
â€¢ $$$ milhÃµes

InferÃªncia (fazemos local):
â€¢ Usar modelo pronto
â€¢ Seu PC/GPU
â€¢ InstantÃ¢neo
â€¢ Gratuito</div>

                    <h3>5. Prompt Engineering:</h3>
                    <div class="demo-section">
                        <h4>âŒ Ruim:</h4>
                        <p>"Me ajuda com cÃ³digo"</p>

                        <h4>âœ… Bom:</h4>
                        <p>"Crie funÃ§Ã£o Python para validar email com regex. Inclua docstring e tratamento de erros."</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 5: Por que IA Local? -->
        <div class="slide">
            <h2>Por que IA Local?</h2>

            <div class="bullet-grid">
                <div>
                    <h3>Vantagens Principais:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Privacidade Total</span> â†’ Seus dados ficam com vocÃª</li>
                        <li><span class="highlight">Sem Limites de Uso</span> â†’ Rode quantas vezes quiser</li>
                        <li><span class="highlight">Zero Custo</span> â†’ Sem APIs pagas</li>
                        <li><span class="highlight">Velocidade</span> â†’ Sem latÃªncia de rede</li>
                        <li><span class="highlight">CustomizaÃ§Ã£o</span> â†’ Adapte para seu workflow</li>
                    </ul>
                </div>
                <div>
                    <h3>Casos de Uso:</h3>
                    <ul class="bullet-list">
                        <li>Code review automÃ¡tico</li>
                        <li>DocumentaÃ§Ã£o inteligente</li>
                        <li>RefatoraÃ§Ã£o de cÃ³digo</li>
                        <li>Debug assistido</li>
                        <li>GeraÃ§Ã£o de testes</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 6: Anatomia dos Modelos -->
        <div class="slide">
            <h2>Modelos de IA</h2>

            <div class="bullet-grid">
                <div>
                    <h3>Conceitos Essenciais:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">ParÃ¢metros:</span> ConexÃµes neurais do modelo (7B, 14B, 32B)</li>
                        <li><span class="highlight">QuantizaÃ§Ã£o:</span> OtimizaÃ§Ã£o (Q4, Q8, FP16)</li>
                        <li><span class="highlight">Context Window:</span> Quanto o modelo "lembra"</li>
                    </ul>
                </div>
                <div>
                    <h3>Regra PrÃ¡tica:</h3>
                    <div class="code-block">VRAM â†’ Modelo Recomendado
8GB          â†’ Qwen2.5-Coder 7B Q4
16GB         â†’ Qwen2.5-Coder 14B Q4
32GB+        â†’ Qwen3Coder 30B Q4</div>
                </div>
            </div>
        </div>

        <!-- Slide 7: CÃ¡lculo de MemÃ³ria -->
        <div class="slide">
            <h2>MatemÃ¡tica dos Modelos</h2>

            <div class="bullet-grid">
                <div>
                    <h3>A FÃ³rmula BÃ¡sica:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">1 parÃ¢metro = 4 bytes</span> (float32)</li>
                        <li><span class="highlight">7 bilhÃµes Ã— 4 bytes = 28 GB</span></li>
                        <li><span class="highlight">Mais overheads = ~30 GB total</span></li>
                    </ul>

                    <div class="demo-section">
                        <h4>ğŸ§® Calculadora RÃ¡pida:</h4>
                        <div class="code-block">Tamanho = ParÃ¢metros Ã— Bytes

Qwen2.5-Coder 7B Ã— 4 = 28 GB (FP32)
Qwen2.5-Coder 14B Ã— 4 = 56 GB (FP32)
Qwen3Coder 30B Ã— 4 = 120 GB (FP32)

ğŸ¤¯ GPT-4: 1.8T Ã— 4 = 7.2 TB!</div>
                    </div>
                </div>
                <div>
                    <h3>Por que Ã© Problema:</h3>
                    <ul class="bullet-list">
                        <li>GPU consumidor: 8-24 GB VRAM</li>
                        <li>Modelo 7B FP32: 28 GB</li>
                        <li><span class="highlight">NÃ£o cabe!</span> ğŸ˜±</li>
                    </ul>

                    <h3>A SoluÃ§Ã£o:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">QuantizaÃ§Ã£o:</span> Reduzir precisÃ£o</li>
                        <li><span class="highlight">Menos bytes = cabe na GPU</span></li>
                        <li>Pequena perda de qualidade</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 8: QuantizaÃ§Ã£o -->
        <div class="slide">
            <h2>QuantizaÃ§Ã£o: Reduzindo PrecisÃ£o</h2>

            <div class="bullet-grid">
                <div>
                    <h3>Tipos de QuantizaÃ§Ã£o:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">FP32:</span> 32 bits = 4 bytes (original)</li>
                        <li><span class="highlight">FP16:</span> 16 bits = 2 bytes (-50%)</li>
                        <li><span class="highlight">Q8:</span> 8 bits = 1 byte (-75%)</li>
                        <li><span class="highlight">Q4:</span> 4 bits = 0.5 bytes (-87.5%)</li>
                    </ul>

                    <h3>Qualidade vs Tamanho:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Q8:</span> Quase sem perda</li>
                        <li><span class="highlight">Q4:</span> Boa qualidade, muito menor</li>
                        <li><span class="highlight">Q2:</span> Existe, mas degrada muito</li>
                    </ul>
                </div>
                <div>
                    <h3>Qwen2.5-Coder 7B na PrÃ¡tica:</h3>
                    <div class="code-block">ğŸ“Š ComparaÃ§Ã£o Real:

FP32: 28 GB â† NÃ£o cabe
FP16: 15 GB â† Cabe em RTX 4090
Q8:  8.1 GB â† Cabe em RTX 3080
Q4:  4.7 GB â† Cabe em GTX 1080

ğŸ¯ Sweet spot: Q4 ou Q8</div>

                    <div class="demo-section">
                        <h4>ğŸ’¡ Regra PrÃ¡tica:</h4>
                        <p><strong>Sua VRAM Ã· 2 = Modelo mÃ¡ximo em Q4</strong><br>
                        Ex: 16 GB â†’ Qwen2.5-Coder 14B Q4</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 9: KV Cache -->
        <div class="slide">
            <h2>KV Cache: A MemÃ³ria dos Modelos</h2>
            <div class="subtitle">"Como os modelos 'lembram' da conversa"</div>

            <div class="bullet-grid">
                <div>
                    <h3>O que Ã© KV Cache:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">K = Keys:</span> "Chaves" das palavras anteriores</li>
                        <li><span class="highlight">V = Values:</span> "Valores" do contexto</li>
                        <li><span class="highlight">Cache:</span> MemÃ³ria temporÃ¡ria rÃ¡pida</li>
                        <li><span class="highlight">Evita recÃ¡lculo:</span> NÃ£o processa tudo novamente</li>
                    </ul>

                    <h3>Por que Ã© importante:</h3>
                    <ul class="bullet-list">
                        <li>Conversas longas = muito KV Cache</li>
                        <li>Mais contexto = mais VRAM usada</li>
                        <li>Limite de tokens = limite de memÃ³ria</li>
                    </ul>
                </div>
                <div>
                    <h3>Exemplo PrÃ¡tico:</h3>
                    <div class="code-block">Context Window: 32K tokens
Qwen2.5-Coder 7B Q4: ~4.7GB base

+ 8K tokens contexto = +400MB
+ 16K tokens contexto = +800MB
+ 32K tokens contexto = +1.6GB

Total: atÃ© 5.1GB VRAM!</div>

                    <div class="demo-section">
                        <h4>ğŸ’¡ Dica PrÃ¡tica:</h4>
                        <p>Context window grande = mais VRAM necessÃ¡ria.<br>
                        Monitore uso de memÃ³ria em conversas longas!</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 10: Arsenal de Ferramentas -->
        <div class="slide">
            <h2>Arsenal de Ferramentas</h2>
            <div class="subtitle">"Suas opÃ§Ãµes para rodar IA local"</div>

            <div class="bullet-grid">
                <div>
                    <h3>Para Iniciantes:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Ollama</span> â†’ Mais simples</li>
                        <li><span class="highlight">LM Studio</span> â†’ Interface grÃ¡fica amigÃ¡vel</li>
                    </ul>

                    <h3>Para Power Users:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">llama.cpp</span> â†’ MÃ¡ximo controle</li>
                        <li><span class="highlight">vLLM</span> â†’ Otimizado para produÃ§Ã£o</li>
                    </ul>
                </div>
                <div>
                    <div class="demo-section">
                      <h3>Interface Web:</h3>
                      <ul class="bullet-list">
                          <li><span class="highlight">Open WebUI</span> â†’ ChatGPT-like</li>
                      </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 11: Instalando Ollama -->
        <div class="slide">
            <h2>Instalando Ollama</h2>

            <div class="bullet-grid">
                <div>
                    <h3>Linux (Nativo):</h3>
                    <div class="code-block"># InstalaÃ§Ã£o rÃ¡pida
curl -fsSL https://ollama.com/install.sh | sh

# Download do modelo
ollama pull qwen2.5-coder:7b

# Testar
ollama run qwen2.5-coder:7b</div>

                    <div class="demo-section">
                        <h4>âœ… Vantagens Linux Nativo:</h4>
                        <p>MÃ¡xima performance â€¢ GPU direto â€¢ Menos overhead</p>
                    </div>
                </div>
                <div>
                    <h3>Docker (Qualquer SO):</h3>
                    <div class="code-block"># CPU Only:
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Nvidia GPU:
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# AMD GPU:
docker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm

# Baixar modelo:
docker exec -it ollama ollama pull qwen2.5-coder:7b</div>

                    <div class="demo-section">
                        <h4>ğŸ³ Vantagens Docker:</h4>
                        <p>Isolamento â€¢ Portabilidade â€¢ FÃ¡cil backup</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 12: Autocomplete -->
        <div class="slide">
            <h2>Autocomplete que Funciona</h2>
            <div class="subtitle">"Continue: O Copilot open source"</div>

            <div class="bullet-grid">
                <div>
                    <h3>Setup no VSCode:</h3>
                    <ul class="bullet-list">
                        <li>1. Instalar extensÃ£o Continue</li>
                        <li>2. Configurar modelo local</li>
                        <li>3. Customizar para seu stack</li>
                    </ul>

                    <div class="demo-section">
                        <h4>ğŸ® Demo:</h4>
                        <p>Autocomplete local em aÃ§Ã£o</p>
                    </div>
                </div>
                <div>
                    <h3>ConfiguraÃ§Ã£o:</h3>
                    <div class="code-block">
name: my-local-configuration
version: 0.0.1
schema: v1

models:
  - name: "Ollama Local"
    model: "AUTODETECT"
    provider: "ollama"
    roles:
      - chat
      - edit
      - apply
      - autocomplete
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 13: Open WebUI -->
        <div class="slide">
            <h2>Interface Web Profissional</h2>
            <div class="subtitle">"Open WebUI: Seu ChatGPT privado"</div>

            <div class="bullet-grid">
                <div>
                    <h3>Por que Open WebUI?</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Interface familiar</span> â†’ Igual ao ChatGPT/Claude</li>
                        <li><span class="highlight">Multi-modelos</span> â†’ Alterne entre diferentes AIs</li>
                        <li><span class="highlight">Conversas organizadas</span> â†’ HistÃ³rico e contextos</li>
                        <li><span class="highlight">ColaboraÃ§Ã£o</span> â†’ Compartilhe com equipe</li>
                        <li><span class="highlight">Extensibilidade</span> â†’ Plugins e customizaÃ§Ãµes</li>
                    </ul>
                </div>
                <div>
                    <h3>Features Destaque:</h3>
                    <ul class="bullet-list">
                        <li>Chat com upload de documentos</li>
                        <li>Code highlighting automÃ¡tico</li>
                        <li>Templates de prompts</li>
                        <li>API integrada</li>
                        <li>Modo escuro/claro</li>
                    </ul>

                    <div class="demo-section">
                        <h4>ğŸ¯ ChatGPT vs Open WebUI</h4>
                        <p>Mesma pergunta, velocidade local, privacidade total</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 13.5: InstalaÃ§Ã£o Open WebUI -->
        <div class="slide">
            <h2>Instalando Open WebUI</h2>

            <div class="bullet-grid">
                <div>
                    <h3>OpÃ§Ã£o 1: Standalone</h3>
                    <p style="font-size: 1.2rem; margin-bottom: 12px; color: #475569;">Use quando jÃ¡ tiver Ollama instalado separadamente</p>

                    <div class="code-block"># Standalone (conecta ao Ollama externo)
docker run -d \
  -p 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main

# Acesse: http://localhost:3000
                    </div>

                    <ul class="bullet-list">
                        <li><span class="highlight">Porta 3000:</span> Interface web</li>
                        <li><span class="highlight">host-gateway:</span> Conecta ao Ollama do host</li>
                        <li><span class="highlight">Volume:</span> Persiste conversas e configs</li>
                    </ul>
                </div>
                <div>
                    <h3>OpÃ§Ã£o 2: Bundle com Ollama</h3>
                    <p style="font-size: 1.2rem; margin-bottom: 12px; color: #475569;">Tudo em um Ãºnico container (GPU required)</p>

                    <div class="code-block"># Bundle: Ollama + Open WebUI
docker run -d \
  -p 3000:8080 \
  --gpus=all \
  -v ollama:/root/.ollama \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:ollama
                    </div>

                    <ul class="bullet-list">
                        <li><span class="highlight">--gpus=all:</span> Acesso Ã  GPU</li>
                        <li><span class="highlight">2 volumes:</span> Modelos + dados</li>
                        <li><span class="highlight">Tudo-em-um:</span> NÃ£o precisa Ollama separado</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 15: CLIs -->
        <div class="slide">
            <h2>Agentes de IA via Terminal</h2>
            <div class="subtitle">"Seu assistente inteligente na linha de comando"</div>

            <div class="bullet-grid">
                <div>
                    <h3>Mais conhecidos:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Claude Code</span> â†’ Mais maduro e consolidado</li>
                        <li><span class="highlight">Codex</span> â†’ Vem ganhando espaÃ§o (OpenAI)</li>
                        <li><span class="highlight">Gemini CLI</span> â†’ CLI open source com uso Free (Pro/Flash)</li>
                    </ul>
                </div>
                <div>
                    <h3>Alternativas OpenSource:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Qwen Code</span> â†’ Especializado em cÃ³digo</li>
                        <li><span class="highlight">Forge Code</span> â†’ Muito rÃ¡pido e integrado ao terminal</li>
                        <li><span class="highlight">OpenCode</span> â†’ Alternativa open source</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 16: CLIs Oficiais (Claude & Codex) -->
        <div class="slide">
            <h2>Claude Code & Codex</h2>

            <div class="bullet-grid">
                <div>
                    <h3>Claude Code (Anthropic):</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Agentic Mode:</span> Executa tarefas complexas</li>
                        <li><span class="highlight">MCP Servers:</span> Conecta ferramentas externas</li>
                        <li><span class="highlight">Slash Commands:</span> Comandos customizÃ¡veis</li>
                        <li><span class="highlight">Context-Aware:</span> Entende projeto inteiro</li>
                    </ul>

                    <div class="code-block"># InstalaÃ§Ã£o
npm install -g @anthropic-ai/claude-code

# Modo interativo (padrÃ£o)
claude</div>

                    <div class="demo-section">
                        <h4>ğŸ’° Custo:</h4>
                        <p>A partir de $20.00 - Claude Pro</p>
                    </div>
                </div>
                <div>
                    <h3>Codex (OpenAI):</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">GPT-5:</span> Modelo avanÃ§ado</li>
                        <li><span class="highlight">Multi-API:</span> OpenAI, Ollama, proxies</li>
                    </ul>

                    <div class="code-block"># InstalaÃ§Ã£o
npm install -g @openai/codex

# Config (~/.codex/config.toml)
[model_providers.ollama]
base_url = "http://localhost:11434/v1"

# Usando Proxy (OpenAI-compatible)
[model_providers.openrouter]
base_url = "https://openrouter.ai/api/v1"
env_key = "OPENROUTER_API_KEY"

# Uso
codex -c model_provider=ollama -m gpt-oss:20b</div>

                    <div class="demo-section">
                        <h4>ğŸ’° Custo:</h4>
                        <p>A partir de $20.00 - ChatGPT Plus</p>
                    </div>
                </div>
            </div>

        </div>

        <!-- Slide 17: CLIs Open Source (Gemini & QwenCode) -->
        <div class="slide">
            <h2>Gemini & QwenCode</h2>

            <div class="bullet-grid">
                <div>
                    <h3>Gemini CLI (Google):</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Free Tier:</span> 60 req/min, 1K req/dia</li>
                        <li><span class="highlight">Custom Commands:</span> Scripts personalizados</li>
                        <li><span class="highlight">Multi-modal:</span> Texto, imagem, cÃ³digo</li>
                        <li><span class="highlight">1M tokens:</span> Context window gigante</li>
                    </ul>

                    <div class="code-block"># InstalaÃ§Ã£o
npm install -g @google/gemini-cli

# Custom command (~/.gemini/commands/review.toml)
prompt = """
Analise este cÃ³digo:
- Performance
- SeguranÃ§a
- Boas prÃ¡ticas
"""

# Uso
gemini review main.py</div>
                </div>
                <div>
                    <h3>QwenCode CLI (Fork Gemini):</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Code-First:</span> Otimizado para cÃ³digo</li>
                        <li><span class="highlight">Free Tier:</span> 60 req/min, 2K req/dia</li>
                        <li><span class="highlight">100% Local:</span> Foca em Qwen</li>
                        <li><span class="highlight">Custom Commands:</span> Mesmo do Gemini</li>
                    </ul>

                    <div class="code-block"># InstalaÃ§Ã£o
npm install -g @qwen-code/qwen-code@latest

# Setup (env vars ou ~/.qwen/config)
OPENAI_BASE_URL="http://localhost:11434/v1"
OPENAI_MODEL="qwen2.5-coder:7b"
OPENAI_API_KEY="your_api_key_here"

# Modo interativo
qwen</div>

                    <div class="demo-section">
                        <h4>ğŸ¯ Quando usar:</h4>
                        <p><strong>Gemini:</strong> Multi-modal, Google API<br>
                        <strong>QwenCode:</strong> 100% local, privacidade total</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 18: Forge Code -->
        <div class="slide">
            <h2>Forge Code</h2>

            <div class="bullet-grid">
                <div>
                    <h3>Por que Forge?</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Super RÃ¡pido:</span> Escrito em Rust</li>
                        <li><span class="highlight">Multi-Agent:</span> Muse (plan) + Forge (execute)</li>
                        <li><span class="highlight">Terminal TUI:</span> Interface interativa no terminal</li>
                    </ul>

                    <div class="demo-section">
                        <h4># InstalaÃ§Ã£o:</h4>
                        <p>npm install -g forgecode@latest</p>
                        <br/>

                        <h4># Modo interativo</h4>
                        <p>forge</p>
                        <br/>

                        <h4># Agentes especializados</h4>
                        <p>/muse   # Planejamento e review</p>
                        <p>/forge  # ExecuÃ§Ã£o de mudanÃ§as</p>
                    </div>
                </div>
                <div>
                    <h3>Setup:</h3>
                    <div class="code-block"># Setup APIs (OpenRouter recomendado)
OPENROUTER_API_KEY=sk-...

# Official OpenAI
OPENAI_API_KEY=your_openai_key_here

# Official Anthropic
ANTHROPIC_API_KEY=your_anthropic_key_here

# OpenAI-Compatible Providers
OPENAI_URL=your_provider_url
OPENAI_API_KEY=your_provider_api_key
                    </div>

                    <h3>Casos de Uso:</h3>
                    <ul class="bullet-list">
                        <li>Git commits automÃ¡ticos com contexto</li>
                        <li>Code review antes de commit</li>
                        <li>GeraÃ§Ã£o de PRs com descriÃ§Ã£o detalhada</li>
                        <li>AnÃ¡lise de diffs complexos</li>
                    </ul>

                </div>
            </div>
        </div>

        <!-- Slide 19: Arquitetura -->
        <div class="slide">
            <h2>Arquitetura Completa</h2>

            <div class="bullet-grid">
                <div>
                    <div class="code-block">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ’» Continue (VSCode)  â”‚
â”‚   Autocomplete em tempo â”‚
â”‚   real no seu cÃ³digo    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ HTTP REST API
           â”‚ localhost:11434
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸŒ Open WebUI         â”‚
â”‚   Interface web chat    â”‚
â”‚   http://localhost:8080 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   âš¡ CLIs & Scripts     â”‚
â”‚   Claude, Codex, Forge  â”‚
â”‚   AutomaÃ§Ã£o terminal    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ API OpenAI-compatible
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸš€ Ollama Engine      â”‚
â”‚   Port: 11434           â”‚
â”‚   API: /v1/chat         â”‚
â”‚   Models: ~/.ollama/    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ¤– Modelo LLM         â”‚
â”‚   Qwen2.5-Coder (7B)    â”‚
â”‚   GPU/CPU Processing    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</div>
                </div>
                <div>
                    <h3>ComunicaÃ§Ã£o:</h3>
                    <ul class="bullet-list">
                        <li><span class="highlight">Protocolo:</span> REST API (OpenAI-compatible)</li>
                        <li><span class="highlight">Endpoint:</span> http://localhost:11434/v1/chat/completions</li>
                        <li><span class="highlight">Modelos:</span> Armazenados em ~/.ollama/models/</li>
                        <li><span class="highlight">Processamento:</span> GPU (CUDA/ROCm) ou CPU</li>
                    </ul>

                    <h3>Fluxo de Dados:</h3>
                    <div class="demo-section">
                        <h4>1. VocÃª escreve cÃ³digo/prompt</h4>
                        <p>â†“</p>
                        <h4>2. Cliente envia para Ollama (porta 11434)</h4>
                        <p>â†“</p>
                        <h4>3. Ollama carrega modelo na RAM/VRAM</h4>
                        <p>â†“</p>
                        <h4>4. InferÃªncia no modelo (GPU/CPU)</h4>
                        <p>â†“</p>
                        <h4>5. Resposta volta para o cliente</h4>
                    </div>
                </div>
            </div>


        </div>

        <!-- Slide 20: Obrigado -->
        <div class="slide">
            <h1>Obrigado!</h1>
            <div class="subtitle">Perguntas?</div>

            <div style="display: flex; flex-direction: column; align-items: center; gap: 40px; margin: 60px 0;">
                <div class="cta-item" style="text-align: center; min-width: 600px;">
                    <h4>ğŸ’¬ Contato</h4>
                    <div class="social-links">
                        <a href="https://github.com/GuilhermeSevero" class="social-link">
                            <svg class="social-icon" viewBox="0 0 24 24">
                                <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.30.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                            </svg>
                            /GuilhermeSevero
                        </a>
                        <a href="https://linkedin.com/in/guilherme-severo" class="social-link">
                            <svg class="social-icon" viewBox="0 0 24 24">
                                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                            </svg>
                            /guilherme-severo
                        </a>
                        <a href="https://instagram.com/guilhermehmsevero" class="social-link">
                            <svg class="social-icon" viewBox="0 0 24 24">
                                <path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.40s-.644-1.44-1.439-1.44z"/>
                            </svg>
                            @guilhermehmsevero
                        </a>
                    </div>
                </div>
            </div>

            <div class="presenter-info">
                <strong>Guilherme Severo</strong><br>
                Development Specialist - Somma IT
            </div>
        </div>

    </div>

    <!-- Navigation -->
    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">â† Anterior</button>
        <button class="nav-btn" onclick="nextSlide()">PrÃ³ximo â†’</button>
    </div>

    <!-- Slide Counter -->
    <div class="slide-counter">
        <span id="current-slide">1</span> / <span id="total-slides">20</span>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;

        document.getElementById('total-slides').textContent = totalSlides;

        function showSlide(n) {
            slides[currentSlide].classList.remove('active');

            // Prevent cycling - stop at first and last slides
            if (n < 0) {
                currentSlide = 0;
            } else if (n >= totalSlides) {
                currentSlide = totalSlides - 1;
            } else {
                currentSlide = n;
            }

            slides[currentSlide].classList.add('active');
            document.getElementById('current-slide').textContent = currentSlide + 1;
        }

        function nextSlide() {
            if (currentSlide < totalSlides - 1) {
                showSlide(currentSlide + 1);
            }
        }

        function previousSlide() {
            if (currentSlide > 0) {
                showSlide(currentSlide - 1);
            }
        }

        // Keyboard navigation
        document.addEventListener('keydown', function(e) {
            if (e.key === 'ArrowRight' || e.key === ' ') {
                nextSlide();
            } else if (e.key === 'ArrowLeft') {
                previousSlide();
            } else if (e.key === 'Home') {
                showSlide(0);
            } else if (e.key === 'End') {
                showSlide(totalSlides - 1);
            }
        });

        // Touch/swipe support for mobile
        let startX = 0;
        let startY = 0;

        document.addEventListener('touchstart', function(e) {
            startX = e.touches[0].clientX;
            startY = e.touches[0].clientY;
        });

        document.addEventListener('touchend', function(e) {
            if (!startX || !startY) return;

            let endX = e.changedTouches[0].clientX;
            let endY = e.changedTouches[0].clientY;

            let diffX = startX - endX;
            let diffY = startY - endY;

            if (Math.abs(diffX) > Math.abs(diffY)) {
                if (diffX > 0) {
                    nextSlide();
                } else {
                    previousSlide();
                }
            }

            startX = 0;
            startY = 0;
        });
    </script>
</body>
</html>
